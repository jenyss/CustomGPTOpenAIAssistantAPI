{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0e2e450-5713-4780-b409-293f9fbe762a",
   "metadata": {},
   "source": [
    "<h1>Custom GPT using OpenAI Assistant API</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a508b1c2-3e88-45ad-8117-17fed2ffe466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import fitz  # PyMuPDF for PDF extraction\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n",
    "from openai import OpenAI\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from uuid import uuid4\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00c8d42b-0051-4c38-9fac-1273139d252b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è Deleted existing ChromaDB collection: assistant_docs\n",
      "‚úÖ Successfully reinitialized ChromaDB with a fresh 'assistant_docs' collection.\n"
     ]
    }
   ],
   "source": [
    "# Environment Setup and Document Embedding\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"Missing OPENAI_API_KEY environment variable.\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "# Path to store the Chroma database\n",
    "persist_directory = \"./chroma_assistant_db\"\n",
    "\n",
    "# Try to create or reset the persistent Chroma client\n",
    "try:\n",
    "    persistent_client = chromadb.PersistentClient(\n",
    "        path=persist_directory, \n",
    "        settings=Settings(allow_reset=True)\n",
    "    )\n",
    "    # Delete existing \"assistant_docs\" collection if present\n",
    "    collections = persistent_client.list_collections()\n",
    "    collection_names = [coll.name for coll in collections]\n",
    "    if \"assistant_docs\" in collection_names:\n",
    "        persistent_client.delete_collection(\"assistant_docs\")\n",
    "        print(\"üóëÔ∏è Deleted existing ChromaDB collection: assistant_docs\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Create a fresh collection\n",
    "    vector_db = persistent_client.get_or_create_collection(\"assistant_docs\")\n",
    "    print(\"‚úÖ Successfully reinitialized ChromaDB with a fresh 'assistant_docs' collection.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during ChromaDB initialization: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f180ce5-c42b-44f1-bc74-d23bdca2d2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Embed Documentation\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# Initialize Chroma PersistentClient\n",
    "persistent_client = chromadb.PersistentClient(path=persist_directory, settings=Settings(allow_reset=True))\n",
    "\n",
    "# Create or Get Collection\n",
    "collection = persistent_client.get_or_create_collection(\"assistant_docs\")\n",
    "\n",
    "# Initialize Chroma Vector Store\n",
    "vector_db = Chroma(\n",
    "    client=persistent_client,\n",
    "    collection_name=\"assistant_docs\",\n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "\n",
    "\n",
    "# File paths\n",
    "pdf_path = \"mm_files/MARMIND_GraphQL_Doc.pdf\"\n",
    "txt_path = \"mm_files/MARMIND_GraphQL_Examples.txt\"\n",
    "\n",
    "# Extract text from PDFs\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    return [(page.get_text(\"text\"), pdf_path, page.number + 1) for page in doc]\n",
    "\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Generic txt chunking\n",
    "def load_text_file(txt_path):\n",
    "    \"\"\"\n",
    "    Generic text file loader that attempts to preserve logical content structure.\n",
    "    \"\"\"\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Return content as a single chunk with metadata\n",
    "    # Let the RecursiveCharacterTextSplitter handle the chunking\n",
    "    # based on its configured chunk_size and separators\n",
    "    return [(content, txt_path, 1)]\n",
    "    \n",
    "\n",
    "txt_text = load_text_file(txt_path)\n",
    "\n",
    "# Combine extracted text\n",
    "doc_chunks = pdf_text + txt_text\n",
    "\n",
    "# Split into Chunks\n",
    "documents = []\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=150,\n",
    "    separators=[\n",
    "        \"\\n\\n\\n\",  # Multiple blank lines (major sections)\n",
    "        \"\\n\\n\",    # Paragraph breaks\n",
    "        \"\\n\",      # Line breaks\n",
    "        \". \",      # Sentences\n",
    "        \" \",       # Words\n",
    "        \"\"         # Characters\n",
    "    ]\n",
    ")\n",
    "\n",
    "for text, source, page in doc_chunks:\n",
    "    for chunk in splitter.split_text(text):\n",
    "        documents.append(\n",
    "            Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\"source\": source, \"page\": page if isinstance(page, int) else -1},\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Assign Unique IDs for Chroma\n",
    "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "\n",
    "# Add documents properly\n",
    "vector_db.add_documents(documents=documents, ids=uuids)\n",
    "\n",
    "\n",
    "#  Define the Actual Retrieval Function (search_docs)\n",
    "def search_docs(query: str):\n",
    "    \"\"\"\n",
    "    Search the \"assistant_docs\" Chroma collection using the query.\n",
    "    Return a single string that includes the retrieved text plus citations.\n",
    "    \"\"\"\n",
    "    # We re-instantiate the same Chroma store\n",
    "    vector_store = Chroma(\n",
    "        client=persistent_client,\n",
    "        collection_name=\"assistant_docs\",\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "    # Perform similarity search\n",
    "    docs = vector_store.similarity_search_with_score(query, k=15)\n",
    "\n",
    "    # Combine text into a single string\n",
    "    combined_text = []\n",
    "    citations = []\n",
    "    for doc, score in docs:\n",
    "        source = doc.metadata.get(\"source\", \"Unknown Source\")\n",
    "        page = doc.metadata.get(\"page\", \"Unknown Page\")\n",
    "        combined_text.append(doc.page_content)\n",
    "        citations.append(f\"Source: {source}, Page: {page}\")\n",
    "\n",
    "    # Return formatted output\n",
    "    all_text = \"\\n\\n\".join(combined_text)\n",
    "    all_citations = \"\\n\".join(citations)\n",
    "\n",
    "    return f\"**Retrieved Text:**\\n{all_text}\\n\\n**Citations:**\\n{all_citations}\"\n",
    "\n",
    "\n",
    "# Create an Assistant with a \"search_docs\" Function\n",
    "# !!! Replace Jenys with your company name. Name your documents e.g. Company_name_REST_API.pdf\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Jenys Assistant (Function Calling)\",\n",
    "    instructions=\"\"\"\n",
    "        You are an AI assistant for Jenys, helping answerd gneral product functionality questions, more specific technical questions related to APIs and scripts and produce code with the help of your knowledge base documents.\n",
    "        When you need additional context or references, call the 'search_docs' function\n",
    "        with a 'query' argument relevant to the user's request.\n",
    "\n",
    "        **Always include citations in the final response.** \n",
    "        When responding, present the retrieved text first, followed by the citations explicitly labeled as \"Sources.\"\n",
    "    \"\"\",\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"search_docs\",\n",
    "                \"description\": \"Search Jenys docs for relevant info and return relevant text (with citations).\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"query\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The user's question or search query about Jenys docs.\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"query\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def execute_query(user_question):\n",
    "\n",
    "    # Option 1 :: Create a Thread, Add a User Message, and Initiate a Run\n",
    "    # Use the manual step-by-step approach (second) if you need more control over the thread before running it.\n",
    "    # thread = client.beta.threads.create()\n",
    "    \n",
    "    # client.beta.threads.messages.create(\n",
    "    #     thread_id=thread.id,\n",
    "    #     role=\"user\",\n",
    "    #     content=user_question\n",
    "    # )\n",
    "    \n",
    "    # run = client.beta.threads.runs.create(\n",
    "    #     thread_id=thread.id,\n",
    "    #     assistant_id=assistant.id,\n",
    "    #     tool_choice=\"auto\"  # Ensures Assistant can execute tools automatically\n",
    "    # )\n",
    "    # Create Thread and Run\n",
    "\n",
    "    # Use create_and_run() if you want simplicity and fewer API calls!\n",
    "    run = client.beta.threads.create_and_run(\n",
    "        assistant_id=assistant.id,\n",
    "        thread={\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": user_question}\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Extract thread_id from run\n",
    "    thread_id = run.thread_id  # Fixes the NameError issue\n",
    "    \n",
    "    # Polling Loop to Process the Completion Correctly\n",
    "    while run.status in [\"queued\", \"in_progress\", \"requires_action\"]:\n",
    "        if run.status == \"requires_action\":\n",
    "            # Handle required actions\n",
    "            tool_calls = run.required_action.submit_tool_outputs.tool_calls\n",
    "            tool_outputs = []\n",
    "            \n",
    "            for tool_call in tool_calls:\n",
    "                # Get the function arguments\n",
    "                function_args = json.loads(tool_call.function.arguments)\n",
    "                \n",
    "                # Execute the search_docs function\n",
    "                if tool_call.function.name == \"search_docs\":\n",
    "                    output = search_docs(function_args[\"query\"])\n",
    "                    tool_outputs.append({\n",
    "                        \"tool_call_id\": tool_call.id,\n",
    "                        \"output\": output\n",
    "                    })\n",
    "            \n",
    "            # Submit all tool outputs back to the assistant\n",
    "            run = client.beta.threads.runs.submit_tool_outputs(\n",
    "                thread_id=thread_id, # For Opt 1, use thread_id=thread.id\n",
    "                run_id=run.id,\n",
    "                tool_outputs=tool_outputs\n",
    "            )\n",
    "        else:\n",
    "            time.sleep(2)  # Sleep before checking again\n",
    "            run = client.beta.threads.runs.retrieve(\n",
    "                thread_id=thread_id, # For Opt 1, use thread_id=thread.id\n",
    "                run_id=run.id\n",
    "            )\n",
    "            \n",
    "    \n",
    "    # Process the Assistant's Completion\n",
    "    # 1. Checks if the assistant used a tool call \n",
    "    # 2. Handles the tool call execution. If search_docs is invoked, it calls search_docs(args[\"query\"])\n",
    "    # 3. Retriggers a new run (client.beta.threads.runs.create(...)) to complete the response.\n",
    "    # 4. Handles multiple iterations of tool calls!\n",
    "    if run.status == \"completed\":\n",
    "        all_messages = client.beta.threads.messages.list(thread_id=thread_id) # For Opt 1, use thread_id=thread.id\n",
    "        assistant_messages = [m for m in all_messages if m.role == \"assistant\"]\n",
    "    \n",
    "        if assistant_messages:\n",
    "            last_message = assistant_messages[-1]  # Get the last assistant response\n",
    "            \n",
    "            # Check if the Assistant called a function\n",
    "            if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "                tool_call = last_message.tool_calls[0]\n",
    "                args = json.loads(tool_call.function.arguments)\n",
    "                tool_name = tool_call.function.name\n",
    "    \n",
    "                # Process the function call automatically\n",
    "                if tool_name == \"search_docs\":\n",
    "                    result = search_docs(args[\"query\"])\n",
    "    \n",
    "                    # Send the result back to the assistant\n",
    "                    client.beta.threads.messages.create(\n",
    "                        thread_id=thread_id, # For Opt 1, use thread_id=thread.id\n",
    "                        role=\"tool\",\n",
    "                        tool_call_id=tool_call.id,\n",
    "                        name=tool_name,\n",
    "                        content=result\n",
    "                    )\n",
    "    \n",
    "                    # Retrieve final completion after tool execution\n",
    "                    run = client.beta.threads.runs.create(\n",
    "                        thread_id=thread_id, # For Opt 1, use thread_id=thread.id\n",
    "                        assistant_id=assistant.id,\n",
    "                        tool_choice=\"auto\"  # ‚úÖ Maintain consistency with initial run\n",
    "                    )\n",
    "    \n",
    "                    # Wait for final response\n",
    "                    while run.status in [\"queued\", \"in_progress\", \"requires_action\"]:\n",
    "                        if run.status == \"requires_action\":\n",
    "                            # Handle required actions (same as above)\n",
    "                            tool_calls = run.required_action.submit_tool_outputs.tool_calls\n",
    "                            tool_outputs = []\n",
    "                            \n",
    "                            for tool_call in tool_calls:\n",
    "                                function_args = json.loads(tool_call.function.arguments)\n",
    "                                if tool_call.function.name == \"search_docs\":\n",
    "                                    output = search_docs(function_args[\"query\"])\n",
    "                                    tool_outputs.append({\n",
    "                                        \"tool_call_id\": tool_call.id,\n",
    "                                        \"output\": output\n",
    "                                    })\n",
    "                            \n",
    "                            run = client.beta.threads.runs.submit_tool_outputs(\n",
    "                                thread_id=thread_id, # For Opt 1, use thread_id=thread.id\n",
    "                                run_id=run.id,\n",
    "                                tool_outputs=tool_outputs\n",
    "                            )\n",
    "                        else:\n",
    "                            time.sleep(2)\n",
    "                            run = client.beta.threads.runs.retrieve(\n",
    "                                thread_id=thread_id, # For Opt 1, use thread_id=thread.id\n",
    "                                run_id=run.id\n",
    "                            )\n",
    "    \n",
    "                    # Get final assistant message\n",
    "                    all_messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "                    assistant_messages = [m for m in all_messages if m.role == \"assistant\"]\n",
    "    \n",
    "                    if assistant_messages:\n",
    "                        final_answer = assistant_messages[-1].content\n",
    "                        print(\"\\n===== Jenys Assistant's Final Reply =====\\n\")\n",
    "                        print(final_answer)\n",
    "                    else:\n",
    "                        print(\"No assistant message found in the thread.\")\n",
    "            else:\n",
    "                # If no tool calls, just print the normal response\n",
    "                content_blocks = last_message.content\n",
    "                if isinstance(content_blocks, list) and content_blocks and hasattr(content_blocks[0], \"type\"):\n",
    "                    if content_blocks[0].type == \"text\" and hasattr(content_blocks[0].text, \"value\"):\n",
    "                        print(\"\\n===== Jenys Assistant's Final Reply =====\\n\")\n",
    "                        print(content_blocks[0].text.value)\n",
    "                    else:\n",
    "                        print(\"No text content found in the assistant's response.\")\n",
    "                else:\n",
    "                    print(\"Unexpected content structure from Assistant API.\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"Run ended with status: {run.status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d9f51a-1f68-4a03-aa33-d18ef53303fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_query(\"Give me exactly 20 GraphQL query examples for MARMIND with the document page they are coming from.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd6a647-a689-44aa-8ea0-4fecf394c673",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
